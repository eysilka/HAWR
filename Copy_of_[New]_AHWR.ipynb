{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eysilka/HAWR/blob/master/Copy_of_%5BNew%5D_AHWR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq860rXEZVNd"
      },
      "source": [
        "# Setup Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PjFQPPAgLg0",
        "outputId": "8124b871-975d-45ad-ac4c-ab67aa2265f0"
      },
      "source": [
        "pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 33.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 38.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.2)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144797 sha256=231bcc3af841282a76bb1cfca87f86a384854fca57021bf6e7d588c8cfa8790e\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqzL1C7lUgKf",
        "outputId": "dac7c549-2afe-4c5b-f612-98ff3769df61"
      },
      "source": [
        "pip install watermark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting watermark\n",
            "  Downloading https://files.pythonhosted.org/packages/60/fe/3ed83b6122e70dce6fe269dfd763103c333f168bf91037add73ea4fe81c2/watermark-2.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from watermark) (5.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (50.3.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->watermark) (4.3.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->watermark) (0.6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->watermark) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
            "Installing collected packages: watermark\n",
            "Successfully installed watermark-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox1PwByfQzfS",
        "outputId": "3c76df09-52e6-498b-b905-d1a1f142acdb"
      },
      "source": [
        "pip install keras_tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_tqdm\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (2.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras_tqdm) (4.41.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (1.19.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_tqdm) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras_tqdm) (1.15.0)\n",
            "Installing collected packages: keras-tqdm\n",
            "Successfully installed keras-tqdm-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwxCnx4VT62S",
        "outputId": "0812736b-670e-4af6-cad4-f0391bce58da"
      },
      "source": [
        "# code for loading the format for the notebook\n",
        "import os\n",
        "\n",
        "# 1. magic to print version\n",
        "# 2. magic so that the notebook will reload external python modules\n",
        "%load_ext watermark\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from tensorflow.python.keras import regularizers\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.optimizers import SGD, Adam, RMSprop, Adamax, Adadelta, Adagrad\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Lambda, GRU, BatchNormalization\n",
        "from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers import LSTM, Reshape, Input, Conv2D, MaxPool2D, Bidirectional\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.activations import relu, sigmoid, softmax\n",
        "import keras.backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,keras,sklearn,tensorflow,cv2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ethen 2020-12-19 12:22:31 \n",
            "\n",
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.19.4\n",
            "pandas 1.1.5\n",
            "keras 2.4.3\n",
            "sklearn 0.0\n",
            "tensorflow 2.4.0\n",
            "cv2 4.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXVIMJ4AZkWb"
      },
      "source": [
        "# Connect My Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhtHh6RNZsyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00b074e-5ba3-4596-b29e-79d3998782fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFBHLDXcaBDa"
      },
      "source": [
        "#with open('parser/words.txt') as f:\n",
        "path = 'drive/My Drive/data [NEW]/'\n",
        "with open(path + 'words.txt') as f:\n",
        "    contents = f.readlines()\n",
        "\n",
        "lines = [line.strip() for line in contents]\n",
        "num_files = len(lines)\n",
        "num_files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj1DVPlIXWlt"
      },
      "source": [
        "# tf-GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPptsb4yRI_J"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#ignore warnings in the output\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqRVqsAIXmcv"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check all available devices if GPU is available\n",
        "print(device_lib.list_local_devices())\n",
        "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIZM03yWXm2v"
      },
      "source": [
        "tf.compat.v1.config.experimental.list_physical_devices('GPU')\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs:\", len(physical_devices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uTvwa83bSrl"
      },
      "source": [
        "# Initializing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlA1fsYn7EpO"
      },
      "source": [
        "char_list = \"ሀሁሂሃሄህሆለሉሊላሌልሎሏሐሑሒሓሔሕሖመሙሚማሜምሞሟሠሡሢሣሤሥሦሧረሩሪራሬርሮሯሰሱሲሳሴስሶሷሸሹሺሻሼሽሾሿቀቁቂቃቄቅቆቋበቡቢባቤብቦቧቨቩቪቫቬቭቮቯተቱቲታቴትቶቷቸቹቺቻቼችቾቿኀኁኂኃኄኅኋነኑኒናኔንኖኗኘኙኚኛኜኝኞኟአኡኢኣኤእኦኧከኩኪካኬክኮኳኸኹኺኻኼኽኾዀዂወዉዊዋዌውዎዏዐዑዒዓዔዕዖዘዙዚዛዜዝዞዟዠዡዢዣዤዥዦዧየዩዪያዬይዮደዱዲዳዴድዶዷጀጁጂጃጄጅጆጇገጉጊጋጌግጎጐጓጠጡጢጣጤጥጦጧጨጩጪጫጬጭጮጯጰጱጲጳጴጵጶጷጸጹጺጻጼጽጾጿፀፁፂፃፄፅፆፈፉፊፋፌፍፎፏፐፑፒፓፔፕፖፗ!፦‹(«፥%»)›.+፣-።/0123456789፡፤…*#?\"\n",
        "len(char_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z-PSOvQ2ckC"
      },
      "source": [
        "RECORDS_COUNT = num_files\n",
        "X_train = []\n",
        "train_labels = []\n",
        "train_input_length = []\n",
        "train_label_length = []\n",
        "\n",
        "X_val = []\n",
        "valid_labels = []\n",
        "valid_input_length = []\n",
        "valid_label_length = []\n",
        "\n",
        "X_test = []\n",
        "test_labels = []\n",
        "test_input_length = []\n",
        "test_label_length = []\n",
        "\n",
        "inputs_length = []\n",
        "labels_length = []\n",
        "max_label_len = 11"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5TOpn1q3C0m"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  for index, line in enumerate(lines):\n",
        "      splits = line.split(' ')\n",
        "      status = splits[1]\n",
        "\n",
        "      if status == 'ok':\n",
        "          word_id = splits[0]\n",
        "          word = \"\".join(splits[8:])\n",
        "\n",
        "          splits_id = word_id.split('-')\n",
        "\n",
        "          if len(word) > max_label_len:\n",
        "              max_label_len = len(word)\n",
        "\n",
        "      if index >= RECORDS_COUNT:\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6lPopkmdgG5"
      },
      "source": [
        "# Generate train & validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehgc-e5leie3"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  # the training, validation and test set\n",
        "  X_train = np.load(path + 'npy/X_train.npy')\n",
        "  train_input_length = np.load(path + 'npy/train_input_length.npy')\n",
        "  train_label_length = np.load(path + 'npy/train_label_length.npy')\n",
        "  X_val = np.load(path + 'npy/X_val.npy')\n",
        "  valid_input_length = np.load(path + 'npy/valid_input_length.npy')\n",
        "  valid_label_length = np.load(path + 'npy/valid_label_length.npy')\n",
        "  X_test = np.load(path + 'npy/X_test.npy')\n",
        "  test_input_length = np.load(path + 'npy/test_input_length.npy')\n",
        "  test_label_length = np.load(path + 'npy/test_label_length.npy')\n",
        "\n",
        "  y_train = np.load(path + 'npy/y_train.npy')\n",
        "  y_val = np.load(path + 'npy/y_val.npy')\n",
        "  y_test = np.load(path + 'npy/y_test.npy')\n",
        "\n",
        "  print()\n",
        "  print('number of training images: ', X_train.shape[0])\n",
        "  print('number of validation images: ', X_val.shape[0])\n",
        "  print('number of testing images: ', X_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AGh9icveWuh"
      },
      "source": [
        "# Build Model\n",
        "Convolutional Recurrent Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bim-kFyHylCC"
      },
      "source": [
        "INPUT_SHAPE = (32, 128, 1)\n",
        "POOL_SIZE = (2, 2)\n",
        "KERNEL_SIZE = (3, 3)\n",
        "KERNEL_SIZE_BN = (2, 2)\n",
        "REG = 0.00001\n",
        "\n",
        "def create_model():\n",
        "  # Initialise a model\n",
        "  model = Sequential()\n",
        "\n",
        "  # First conv layer - input layer\n",
        "  model.add(Convolution2D(filters=64, kernel_size = KERNEL_SIZE, input_shape=INPUT_SHAPE,\n",
        "                 use_bias=True,\n",
        "                 strides=(1, 1),\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv1'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=POOL_SIZE, strides=(2,2), name='pool1'))\n",
        "\n",
        "  model.add(Convolution2D(filters=128, kernel_size = KERNEL_SIZE,\n",
        "                 strides=(1, 1),\n",
        "                 use_bias=True,\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv2'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(MaxPooling2D(pool_size=POOL_SIZE, strides=(2,2), name='pool2'))\n",
        "\n",
        "  model.add(Convolution2D(filters=256, kernel_size = KERNEL_SIZE,\n",
        "                 strides=(1, 1),\n",
        "                 use_bias=True,\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv3'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(Convolution2D(filters=256, kernel_size = KERNEL_SIZE,\n",
        "                 strides=(1, 1),\n",
        "                 use_bias=True,\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv4'))\n",
        "  model.add(Activation('relu'))\n",
        "  # pooling layer with kernel size (2,1)\n",
        "  model.add(MaxPooling2D(pool_size=(2, 1), name='pool3'))\n",
        "\n",
        "  model.add(Convolution2D(filters=512, kernel_size = KERNEL_SIZE,\n",
        "                 strides=(1, 1),\n",
        "                 use_bias=True,\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv5'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  # Batch normalization layer\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Convolution2D(filters=512, kernel_size = KERNEL_SIZE,\n",
        "                 strides=(1, 1),\n",
        "                 use_bias=True,\n",
        "                 padding='same', kernel_regularizer=regularizers.l2(REG), name='conv6'))\n",
        "  model.add(Activation('relu'))\n",
        "\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 1), name='pool4'))\n",
        "\n",
        "  model.add(Convolution2D(filters=512, kernel_size = KERNEL_SIZE_BN,\n",
        "                          kernel_regularizer=regularizers.l2(REG), name='conv7'))\n",
        "  model.add(Activation('relu'))\n",
        "  #model.add(Dense(64, activation='relu', name='dense1'))\n",
        "\n",
        "  model.add(Lambda(lambda x: K.squeeze(x, 1)))\n",
        "\n",
        "  # bidirectional LSTM layers with units=128\n",
        "  #model.add(Bidirectional(LSTM(256, return_sequences=True,\n",
        "  #                             kernel_regularizer=regularizers.l2(REG), name='blstm1')))\n",
        "  #model.add(Dropout(0.2))\n",
        "  with tf.device('/gpu:0'):\n",
        "    model.add(Bidirectional(GRU(512, return_sequences=True,\n",
        "                  kernel_regularizer=regularizers.l2(REG), name='gru1')))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "  #model.add(Bidirectional(LSTM(256, return_sequences=True,\n",
        "  #                             kernel_regularizer=regularizers.l2(REG), name='blstm2')))\n",
        "  #model.add(Dropout(0.2))\n",
        "  with tf.device('/gpu:1'):\n",
        "    model.add(Bidirectional(GRU(256, return_sequences=True,\n",
        "                  kernel_regularizer=regularizers.l2(REG), name='gru2')))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # transforms RNN output to character activations\n",
        "  model.add(Dense(len(char_list)+1,\n",
        "                  kernel_regularizer=regularizers.l2(REG), name='outputs'))\n",
        "  model.add(Activation('softmax', name='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "my_model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXA6dMoo1MWA"
      },
      "source": [
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGI_043qfGAa"
      },
      "source": [
        "batch_size = 8\n",
        "epochs = 200\n",
        "e = str(epochs)\n",
        "#optimizer_name = 'adadelta'\n",
        "#opt = RMSProp(learning_rate=0.001)\n",
        "import keras\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "opt = Adamax(learning_rate=lr_schedule, name='adamax')\n",
        "#opt='adam'\n",
        "inputs = my_model.input\n",
        "outputs = my_model.output\n",
        "greedy=True\n",
        "beam_width=100\n",
        "top_paths=1\n",
        "charset=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB8I5how47Jh"
      },
      "source": [
        "from tensorflow.python.ops import ctc_ops as ctc, sparse_ops, array_ops\n",
        "def Kreshape_To1D(my_tensor):\n",
        "    \"\"\" Reshape to a 1D Tensor using K.reshape\"\"\"\n",
        "\n",
        "    sum_shape = K.sum(K.shape(my_tensor))\n",
        "    return K.reshape(my_tensor, (sum_shape,))\n",
        "\n",
        "\n",
        "def tf_edit_distance(hypothesis, truth, norm=False):\n",
        "    \"\"\" Edit distance using tensorflow\n",
        "    inputs are tf.Sparse_tensors \"\"\"\n",
        "\n",
        "    return tf.edit_distance(hypothesis, truth, normalize=norm, name='edit_distance')\n",
        "\n",
        "def ctc_loss_lambda_func(args):\n",
        "    \"\"\"\n",
        "    Function for computing the ctc loss (can be put in a Lambda layer)\n",
        "    :param args:\n",
        "        y_pred, labels, input_length, label_length\n",
        "    :return: CTC loss\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)  # , ignore_longer_outputs_than_inputs=True)\n",
        "\n",
        "\n",
        "def ctc_complete_decoding_lambda_func(args, **arguments):\n",
        "    \"\"\"\n",
        "    Complete CTC decoding using Keras (function K.ctc_decode)\n",
        "    :param args:\n",
        "        y_pred, input_length\n",
        "    :param arguments:\n",
        "        greedy, beam_width, top_paths\n",
        "    :return:\n",
        "        K.ctc_decode with dtype='float32'\n",
        "    \"\"\"\n",
        "\n",
        "    # import tensorflow as tf # Require for loading a model saved\n",
        "\n",
        "    y_pred, input_length = args\n",
        "    my_params = arguments\n",
        "\n",
        "    assert (K.backend() == 'tensorflow')\n",
        "\n",
        "    return K.cast(K.ctc_decode(y_pred, tf.squeeze(input_length), greedy=my_params['greedy'],\n",
        "                                beam_width=my_params['beam_width'], top_paths=my_params['top_paths'])[0][0],\n",
        "                  dtype='float32')\n",
        "\n",
        "\n",
        "def ctc_complete_analysis_lambda_func(args, **arguments):\n",
        "    \"\"\"\n",
        "    Complete CTC analysis using Keras and tensorflow\n",
        "    WARNING : tf is required\n",
        "    :param args:\n",
        "        y_pred, labels, input_length, label_len\n",
        "    :param arguments:\n",
        "        greedy, beam_width, top_paths\n",
        "    :return:\n",
        "        ler = label error rate\n",
        "    \"\"\"\n",
        "\n",
        "    # import tensorflow as tf # Require for loading a model saved\n",
        "\n",
        "    y_pred, labels, input_length, label_len = args\n",
        "    my_params = arguments\n",
        "\n",
        "    assert (K.backend() == 'tensorflow')\n",
        "\n",
        "    batch = tf.math.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n",
        "    input_length = tf.cast(tf.squeeze(input_length), tf.int32)\n",
        "\n",
        "    greedy = my_params['greedy']\n",
        "    beam_width = my_params['beam_width']\n",
        "    top_paths = my_params['top_paths']\n",
        "\n",
        "    if greedy:\n",
        "        (decoded, log_prob) = ctc.ctc_greedy_decoder(\n",
        "            inputs=batch,\n",
        "            sequence_length=input_length)\n",
        "    else:\n",
        "        (decoded, log_prob) = ctc.ctc_beam_search_decoder(\n",
        "            inputs=batch, sequence_length=input_length,\n",
        "            beam_width=beam_width, top_paths=top_paths)\n",
        "\n",
        "    cast_decoded = tf.cast(decoded[0], tf.float32)\n",
        "\n",
        "    sparse_y = K.ctc_label_dense_to_sparse(labels, tf.cast(tf.squeeze(label_len), tf.int32))\n",
        "    ed_tensor = tf_edit_distance(cast_decoded, sparse_y, norm=True)\n",
        "    ler_per_seq = Kreshape_To1D(ed_tensor)\n",
        "\n",
        "    return K.cast(ler_per_seq, dtype='float32')\n",
        "# Others inputs for the CTC approach\n",
        "labels = Input(name='labels', shape=[None])\n",
        "input_length = Input(name='input_length', shape=[1])\n",
        "label_length = Input(name='label_length', shape=[1])\n",
        "\n",
        "# Lambda layer for computing the loss function\n",
        "loss_out = Lambda(ctc_loss_lambda_func, output_shape=(1,), name='CTCloss')([outputs, labels, input_length, label_length])\n",
        "\n",
        "\n",
        "# Lambda layer for the decoding function\n",
        "out_decoded_dense = Lambda(ctc_complete_decoding_lambda_func, output_shape=(None, None), name='CTCdecode',\n",
        "                            arguments={'greedy': greedy,\n",
        "                                      'beam_width': beam_width, 'top_paths': top_paths},\n",
        "                            dtype=\"float32\")([outputs, input_length])\n",
        "\n",
        "# Lambda layer to perform an analysis (CER and SER)\n",
        "out_analysis = Lambda(ctc_complete_analysis_lambda_func, output_shape=(None,), name='CTCanalysis',\n",
        "                      arguments={'greedy': greedy,\n",
        "                                  'beam_width': beam_width, 'top_paths': top_paths}, dtype=\"float32\")([outputs, labels, input_length, label_length])\n",
        "# create Keras models\n",
        "model_init = Model(inputs=inputs, outputs=outputs)\n",
        "model_train = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
        "model_pred = Model(inputs=[inputs, input_length], outputs=out_decoded_dense)\n",
        "model_eval = Model(inputs=[inputs, labels, input_length, label_length], outputs=out_analysis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZYZsApyBmat"
      },
      "source": [
        "# Compile models\n",
        "model_train.compile(loss={'CTCloss': lambda yt, yp: yp}, optimizer=opt, metrics=['accuracy'])\n",
        "model_pred.compile(loss={'CTCdecode': lambda yt, yp: yp}, optimizer=opt, metrics=['accuracy'])\n",
        "model_eval.compile(loss={'CTCanalysis': lambda yt, yp: yp}, optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9LJ6KucfOqJ"
      },
      "source": [
        "#model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=opt, metrics=['accuracy'])\n",
        "filepath= path + \"models/{}o-{}r-{}e-{}t-{}v.hdf5\".format(opt,\n",
        "                                                          str(RECORDS_COUNT),\n",
        "                                                          str(epochs),\n",
        "                                                          str(X_train.shape[0]),\n",
        "                                                          str(X_val.shape[0]))\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "#callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDFikYPxmj7k"
      },
      "source": [
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta = 0.1,\n",
        "    patience = 20)\n",
        "with tf.device('/gpu:0'):\n",
        "  history = model_train.fit(x=[X_train, y_train, train_input_length, train_label_length],\n",
        "                      y=np.zeros(len(X_train)),\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs,\n",
        "                      #validation_split=0.2,\n",
        "                      validation_data=([X_val, y_val, valid_input_length, valid_label_length], [np.zeros(len(X_val))]),\n",
        "                      verbose=2,\n",
        "                      callbacks=[early_stop, checkpoint]\n",
        "                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Epc3XBfRHQ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaYmgiHRfvfu"
      },
      "source": [
        "# plot accuracy and loss\n",
        "def plotgraph(epochs, acc, val_acc):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(epochs, acc, 'b')\n",
        "    plt.plot(epochs, val_acc, 'r')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Val'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9QRkT4ga6Z"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1,len(loss)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy0kAaHggdv6"
      },
      "source": [
        "plt.title('Model loss')\n",
        "plotgraph(epochs, loss, val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHjAY0CrggpN"
      },
      "source": [
        "plt.title('Model accuracy')\n",
        "plotgraph(epochs, acc, val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf4IPJGTzSjJ"
      },
      "source": [
        "print(\"\\tMean train cost: \",np.mean(val_loss))\n",
        "print(\"\\tMean train perplexity: \",np.mean(list(map(lambda x: np.power(np.e,x), val_loss))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhN0HHdPgjmk"
      },
      "source": [
        "# get best model index\n",
        "minimum_val_loss = np.min(history.history['val_loss'])\n",
        "best_model_index = np.where(history.history['val_loss'] == minimum_val_loss)[0][0]\n",
        "\n",
        "best_loss = str(history.history['loss'][best_model_index])\n",
        "best_acc = str(history.history['accuracy'][best_model_index])\n",
        "best_val_loss = str(history.history['val_loss'][best_model_index])\n",
        "best_val_acc = str(history.history['val_accuracy'][best_model_index])\n",
        "\n",
        "best_loss,best_acc,best_val_loss,best_val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "sdkuXPI_X1Ur"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPAGVF91gB6U"
      },
      "source": [
        "# Test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_jyhC_PgfvH"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  model_train.load_weights(filepath)\n",
        "  train_data = ([X_train, y_train, train_input_length, train_label_length], [np.zeros(len(X_train))])\n",
        "  valid_data = ([X_val, y_val, valid_input_length, valid_label_length], [np.zeros(len(X_val))])\n",
        "  test_data = ([X_test, y_test, test_input_length, test_label_length], [np.zeros(len(X_test))])\n",
        "  scores1 = model_train.evaluate(x=train_data[0], y=train_data[1], verbose=1)\n",
        "  scores2 = model_train.evaluate(x=valid_data[0], y=valid_data[1], verbose=1)\n",
        "  scores3 = model_train.evaluate(x=test_data[0], y=test_data[1], verbose=1)\n",
        "\n",
        "  print('\\nTrain accuracy: \\t %.3f \\t loss: %.3f' % (scores1[1]*100,scores1[0]))\n",
        "\n",
        "  print('\\nValidation accuracy: \\t %.3f \\t loss: %.3f' % (scores2[1]*100,scores2[0]))\n",
        "\n",
        "  print('\\nTest accuracy:  \\t %.3f \\t loss: %.3f' % (scores3[1]*100,scores3[0]))\n",
        "\n",
        "  print(\"\\nBaseline Error: \\t %.2f%%\" % (100-scores3[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZomwPu0WQSMy"
      },
      "source": [
        "model_pred.set_weights(model_train.get_weights())\n",
        "model_eval.set_weights(model_train.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvQRwUftTEo6"
      },
      "source": [
        "# 'ler' : compute the label error rate\n",
        "# 'ser' : compute the sequence error rate\n",
        "seq_error = 0\n",
        "nb_data = X_test.shape[0]\n",
        "eval_batch = model_eval.predict(x=test_data[0])\n",
        "seq_error += np.sum([1 for ler_data in eval_batch if ler_data != 0])\n",
        "seq_error = seq_error / nb_data if nb_data > 0 else -1.\n",
        "\n",
        "outmetrics = []\n",
        "#outmetrics.append(eval_batch)\n",
        "outmetrics.append(seq_error)\n",
        "print('character error rate: %.3f%%' % (outmetrics[0]*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI8AnHa-hPoi"
      },
      "source": [
        "# More Reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF1MSgbxgp3i"
      },
      "source": [
        "Using Jaro Distance & Ratio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd7WgavincAe"
      },
      "source": [
        "def recognized(x):\n",
        "  s=''\n",
        "  for p in x:\n",
        "    if int(p) != 300:\n",
        "      if int(p) != -1:\n",
        "        s += char_list[int(p)] + ''\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5nfd0JcxLBn"
      },
      "source": [
        "def wer(r, h):\n",
        "    \"\"\"\n",
        "    Calculation of WER with Levenshtein distance.\n",
        "    Works only for iterables up to 254 elements (uint8).\n",
        "    O(nm) time ans space complexity.\n",
        "    Parameters\n",
        "    ----------\n",
        "    r : list\n",
        "    h : list\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "    Examples\n",
        "    --------\n",
        "    >>> wer(\"who is there\".split(), \"is there\".split())\n",
        "    1\n",
        "    >>> wer(\"who is there\".split(), \"\".split())\n",
        "    3\n",
        "    >>> wer(\"\".split(), \"who is there\".split())\n",
        "    3\n",
        "    \"\"\"\n",
        "    # initialisation\n",
        "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8)\n",
        "    d = d.reshape((len(r)+1, len(h)+1))\n",
        "    for i in range(len(r)+1):\n",
        "        for j in range(len(h)+1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                d[i][j] = d[i-1][j-1]\n",
        "            else:\n",
        "                substitution = d[i-1][j-1] + 1\n",
        "                insertion = d[i][j-1] + 1\n",
        "                deletion = d[i-1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "    return d[len(r)][len(h)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JTblQHPuN0B"
      },
      "source": [
        "def jaro_and_ratio(letters, original_text):\n",
        "  \"\"\"In computer science and statistics,\n",
        "  the Jaro–Winkler distance is a string metric measuring an edit distance\n",
        "  between two sequences. It is a variant proposed in 1990 by William E. Winkler\n",
        "  of the Jaro distance metric (1989, Matthew A. Jaro).\"\"\"\n",
        "  total_jar = 0\n",
        "  total_rati = 0\n",
        "\n",
        "  for i in range(len(letters)):\n",
        "    total_jar+=lv.jaro(letters[i], original_text[i])\n",
        "    total_rati+=lv.ratio(letters[i], original_text[i])\n",
        "\n",
        "  jaro = total_jar/len(letters)\n",
        "  ratio = total_rati/len(letters)\n",
        "  print(len(letters))\n",
        "  return jaro, ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3WEiyv1c33"
      },
      "source": [
        "def find_accuracy(true_chars, pred_chars):\n",
        "  print(\"true\", true_chars)\n",
        "  print(\"pred\", pred_chars)\n",
        "  good = 0\n",
        "  total = 0\n",
        "  good_word_count = 0\n",
        "  total_words = 0\n",
        "  word_size_stats = {}\n",
        "  for i in range(len(true_chars)):\n",
        "    good_word = True\n",
        "    good_c = 0\n",
        "    for c in range(len(true_chars[i])):\n",
        "      try:\n",
        "        if true_chars[i][c] == pred_chars[i][c]:\n",
        "          good += 1\n",
        "          good_c += 1\n",
        "        else:\n",
        "          good_word = False\n",
        "      except:\n",
        "        total += 1\n",
        "        good_word = False\n",
        "        continue\n",
        "      total += 1\n",
        "\n",
        "    if good_word:\n",
        "      good_word_count += 1\n",
        "    total_words += 1\n",
        "    if len(true_chars[i]) in word_size_stats:\n",
        "      word_size_stats[len(true_chars[i])].append((good_c+0.0)/len(true_chars[i]))\n",
        "    else:\n",
        "      word_size_stats[len(true_chars[i])] = [(good_c+0.0)/len(true_chars[i])]\n",
        "  return ((good + 0.0) / total, (good_word_count + 0.0)/total_words, word_size_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV01tWXXbuVz"
      },
      "source": [
        "def validate(x, y):\n",
        "  \"\"\" Validate neural network \"\"\"\n",
        "  numCharErr = 0\n",
        "  numCharTotal = 0\n",
        "  numWordOK = 0\n",
        "  numWordTotal = 0\n",
        "\n",
        "  totalCER = []\n",
        "  totalWER = []\n",
        "  x_pred = model_init.predict(x)\n",
        "  x_deco = K.ctc_decode(x_pred , input_length=np.ones(x_pred.shape[0])*x_pred.shape[1], greedy=True)[0][0]\n",
        "  x_reco = K.get_value(x_deco)\n",
        "  x_reco_txt = []\n",
        "  y_orig_txt = []\n",
        "  for i, j in enumerate(x_reco):\n",
        "    try:\n",
        "      x_reco_txt.append(recognized(j))\n",
        "      y_orig_txt.append(recognized(y[i]))\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  print('Ground truth ~> Recognized')\n",
        "  for i in range(len(x_reco_txt)):\n",
        "      numWordOK += 1 if y_orig_txt[i] == x_reco_txt[i] else 0\n",
        "      numWordTotal += 1\n",
        "      dist = editdistance.eval(x_reco_txt[i], y_orig_txt[i])\n",
        "      ## editdistance\n",
        "      currCER = dist/max(len(x_reco_txt[i]), len(y_orig_txt[i]))\n",
        "      totalCER.append(currCER)\n",
        "\n",
        "      currWER = wer(x_reco_txt[i].split(), y_orig_txt[i].split())\n",
        "      totalWER.append(currWER)\n",
        "\n",
        "      numCharErr += dist\n",
        "      numCharTotal += len(y_orig_txt[i])\n",
        "      #print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' +\n",
        "      #      y_orig_txt[i] + '\"', '~>', '\"' + x_reco_txt[i] + '\"')\n",
        "      with tf.device('/gpu:0'):\n",
        "        if dist != 0:\n",
        "          print('[Err: %d]' % dist, '\"' +\n",
        "                y_orig_txt[i] + '\"', '~>', '\"' + x_reco_txt[i] + '\"')\n",
        "\n",
        "          plt.imshow(x[i].reshape(32,128), cmap=plt.cm.gray)\n",
        "          plt.show()\n",
        "          print('\\n')\n",
        "\n",
        "  # Print validation result\n",
        "  charErrorRate = sum(totalCER)/len(totalCER)\n",
        "  addressAccuracy = numWordOK / numWordTotal\n",
        "  wordErrorRate = sum(totalWER)/len(totalWER)\n",
        "  jaro, ratio = jaro_and_ratio(x_reco_txt, y_orig_txt)\n",
        "  print('Character error rate: %f%%. Address accuracy: %f%%. Word error rate: %f%%' %\n",
        "        (charErrorRate*100.0, addressAccuracy*100.0, wordErrorRate*100.0))\n",
        "  print('jaro: %f%% and ratio: %f%%' % (jaro*100.0, ratio*100.0))\n",
        "\n",
        "  char_acc, word_acc, word_len_distr = find_accuracy(y_orig_txt, x_reco_txt)\n",
        "  print(\"char accuracy: \" + str(char_acc))\n",
        "  print(\"word accuracy: \" + str(word_acc))\n",
        "  print(\"word length analysis:\" + str(word_len_distr))\n",
        "\n",
        "  return charErrorRate, addressAccuracy, wordErrorRate, jaro, ratio\n",
        "\n",
        "import editdistance\n",
        "import Levenshtein as lv\n",
        "\n",
        "# Validate\n",
        "#print('Validate neural network')\n",
        "#charErrorRate, addressAccuracy, wordErrorRate, jaro, ratio = validate(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p77UKvYxHmsR"
      },
      "source": [
        "# Validate\n",
        "print('Validate neural network')\n",
        "charErrorRate, addressAccuracy, wordErrorRate, jaro, ratio = validate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2480wwrg_zv"
      },
      "source": [
        "# Printing a graph showing the accuracy changes during the training phase\n",
        "print(history.history.keys())\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "loss_df = pd.DataFrame(data=history.history)\n",
        "loss_df.head()\n",
        "\n",
        "loss_df.sort_values(by=['loss', 'val_loss']).head()\n",
        "print('\\n')\n",
        "loss_df.sort_values(by=['val_loss', 'loss']).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXZQaC-ziYO7"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(14,7)})\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFBPdIuBgu1E"
      },
      "source": [
        "# Save History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxitf32mgnTt"
      },
      "source": [
        "#drive/My Drive/Colab Notebooks/Handwritten-Text-Recognition/\n",
        "with open(path + 'history.txt', 'a') as f:\n",
        "    new_data = '{},{},{},{},{},{},{},{},{},{}\\n'.format(filepath,\n",
        "                                                      opt,\n",
        "                                                      str(RECORDS_COUNT),\n",
        "                                                      e,\n",
        "                                                      str(X_train.shape[0]),\n",
        "                                                      str(X_val.shape[0]),\n",
        "                                                      best_loss,\n",
        "                                                      best_acc,\n",
        "                                                      best_val_loss,\n",
        "                                                      best_val_acc)\n",
        "    f.write(new_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D9PCq4Zjo2x"
      },
      "source": [
        "#model_1 = KerasClassifier(build_fn=create_model)\n",
        "#grid_result = grid.fit(X_train, y_train)\n",
        "# Parameters we are intersted in tuning\n",
        "#param_grid = dict( optimizer=['SGD','RMSprop'])\n",
        "\n",
        "#grid = GridSearchCV(estimator=model_1, param_grid=param_grid, n_jobs=1,cv=2,verbose=2)\n",
        "#grid_result = grid.fit(X_train, y_train, verbose=2, epochs=3)\n",
        "\n",
        "# Evaluate testing set\n",
        "#test_accuracy = grid.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEw-eFQRzzv4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}